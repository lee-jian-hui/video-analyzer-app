name: Build Windows Installer End to End


on:
  workflow_dispatch:
  push:
    branches:
      - '**'
jobs:
  build-windows:
    runs-on: windows-latest

    steps:
      # --- Step 1. Checkout repo
      - name: Checkout repository
        uses: actions/checkout@v4

      # --- Step 2. Setup Node and Rust (for Tauri)
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      # --- Step 3. Setup Python & PyInstaller
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install PyInstaller
        run: pip install pyinstaller

      # --- Step 4. Build Python backend EXE
      - name: Build backend with PyInstaller
        working-directory: video-analyser-backend
        run: >
          Write-Host "=== Building Python backend ==="
          Get-Location
          Get-ChildItem

          pyinstaller
          --name video_analyzer_backend
          --onedir
          --noconsole
          --add-data "templates;templates"
          --add-data "protos;protos"
          server.py

      - name: Debug backend build output
        shell: pwsh
        run: |
          Write-Host "=== Checking backend build output ==="
          Get-ChildItem -Recurse -Directory -Path video-analyser-backend\dist

      # --- Step 5. Fetch Ollama binary
      - name: Download Ollama EXE from release
        shell: pwsh
        run: |
          Invoke-WebRequest -Uri "https://github.com/lee-jian-hui/video-analyzer-app/releases/download/large-files/ollama.exe" -OutFile "ollama.exe"
          New-Item -ItemType Directory -Force -Path "my-tauri-app/src-tauri/sidecars" | Out-Null
          Move-Item -Force ollama.exe "my-tauri-app/src-tauri/sidecars/"

      - name: Download Ollama models
        shell: pwsh
        run: |
          Invoke-WebRequest -Uri "https://github.com/lee-jian-hui/video-analyzer-app/releases/download/large-files/ollama-models.zip" -OutFile "ollama_models.zip"
          New-Item -ItemType Directory -Force -Path "my-tauri-app/src-tauri/resources/ollama_models" | Out-Null
          Expand-Archive -Force "ollama_models.zip" "my-tauri-app/src-tauri/resources/ollama_models"

      - name: Debug file structure (depth 3)
        shell: pwsh
        run: |
          Write-Host "Current directory: $(Get-Location)"
          Write-Host "`n=== Directory Tree (Depth: 3) ==="
          Get-ChildItem -Recurse -Depth 3 | ForEach-Object {
            $indent = '  ' * ($_.FullName.Split('\').Count - (Get-Location).Path.Split('\').Count)
            Write-Host "$indent- $($_.Name)"
          }

      # --- Step 7. Copy backend EXE into Tauri sidecars
      - name: Copy backend to Tauri sidecars
        shell: pwsh
        run: |
          Write-Host "=== Copying backend to Tauri sidecars ==="

          # Ensure target directory exists
          New-Item -ItemType Directory -Force -Path "my-tauri-app/src-tauri/sidecars/video_analyzer_backend" | Out-Null

          # Copy built backend EXE + dependencies
          Copy-Item -Recurse -Force "video-analyser-backend/dist/video_analyzer_backend/*" "my-tauri-app/src-tauri/sidecars/video_analyzer_backend"

          Write-Host "âœ… Backend successfully copied into Tauri sidecars"
          Get-ChildItem -Recurse "my-tauri-app/src-tauri/sidecars/video_analyzer_backend"

      # --- Step 8. Install frontend dependencies
      - name: Install NPM dependencies
        working-directory: my-tauri-app
        run: npm ci

      # --- Step 9. Build full Tauri Windows installer
      - name: Build Tauri installer
        working-directory: my-tauri-app
        run: npm run tauri build

      # --- Step 10. Upload installer as artifact
      - name: Upload built installer
        uses: actions/upload-artifact@v4
        with:
          name: video-analyzer-windows-installer
          path: my-tauri-app/src-tauri/target/release/bundle/**
