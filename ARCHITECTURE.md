# Architecture Overview: Offline Video AI Desktop (Tauri + gRPC + MCP + Local Llama)

## 1. Overview

This architecture enables a **fully offline**, modular, and extensible desktop AI application built with **Tauri**, **Rust**, and **Python MCP servers**, augmented by a **local Llama 3.2 model** for intelligent orchestration and reasoning.

---

## 2. High-Level System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        React (UI)         â”‚
â”‚  - File upload UI         â”‚
â”‚  - Display transcriptions â”‚
â”‚  - Interactive LLM chat   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Rust Bridge (Tauri)    â”‚
â”‚  - Exposes commands:         â”‚
â”‚    â€¢ upload_video()          â”‚
â”‚    â€¢ process_query()         â”‚
â”‚    â€¢ get_processing_status() â”‚
â”‚  - Calls Python backend via  â”‚
â”‚    gRPC client               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Python Backend (gRPC Orchestrator)        â”‚
â”‚-----------------------------------------------â”‚
â”‚  â€¢ server.py (entry point)                    â”‚
â”‚  â€¢ Implements gRPC service methods            â”‚
â”‚  â€¢ Delegates tasks to local MCP servers       â”‚
â”‚  â€¢ Hosts local Llama 3.2 agent with MCP       â”‚
â”‚-----------------------------------------------â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚    â”‚ MCP Servers (fastmcp)              â”‚     â”‚
â”‚    â”‚ â€¢ Whisper MCP â†’ transcription      â”‚     â”‚
â”‚    â”‚ â€¢ YOLO MCP â†’ object detection      â”‚     â”‚
â”‚    â”‚ â€¢ LLM MCP â†’ summarization          â”‚     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                               â”‚
â”‚    Storage: local SQLite + blob directory     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Design Goals

| Goal                       | Implementation                                                    |
| -------------------------- | ----------------------------------------------------------------- |
| **Offline operation**      | All models and tools run locally â€” no external APIs               |
| **Process isolation**      | Each AI service (Whisper, YOLO, etc.) runs as its own MCP process |
| **Standardized interface** | All MCP servers follow the Model Context Protocol (MCP)           |
| **Dynamic extensibility**  | Add new tools (MCP servers) without modifying core orchestrator   |
| **Interoperability**       | gRPC used between Rust and Python for stability and type safety   |

---

## 4. Core Components

### 4.1 Tauri (Rust)

* Handles user actions and forwards requests to Python gRPC service.
* Uses tonic-generated stubs (`VideoProcessorServiceClient`).
* Calls async functions like:

  ```rust
  client.upload_video(tonic::Request::new(req)).await
  ```
* Built into the final desktop binary using `tauri build`.

---

### 4.2 Python Orchestrator (`server.py`)

* Implements gRPC service endpoints for:

  * `UploadVideo`
  * `ProcessQuery`
  * `GetProcessingStatus`

* On startup:

  1. Loads local Llama 3.2 (via Hugging Face).
  2. Starts or connects to MCP servers (Whisper, YOLO, etc.).
  3. Performs MCP handshake and tool registration.

* Handles logic like:

  ```python
  result = llama_agent.call_tool("transcribe_audio", {"file_path": "/data/uploads/test.mp4"})
  ```

---

### 4.3 MCP Servers (FastMCP)

Each model is encapsulated in an independent MCP server using the `mcp` Python package.

Examples:

* **Whisper MCP** â†’ audio/video transcription
* **YOLO MCP** â†’ object detection
* **Summarizer MCP** â†’ text summarization

Each exposes one or more tools via:

```python
@server.tool()
def transcribe_audio(file_path: str) -> dict:
    ...
```

---

## 5. Data Flow Example

1âƒ£ **Frontend Uploads Video**
â†’ Calls Rust `upload_video()` command
â†’ gRPC â†’ Python orchestrator

2âƒ£ **Python Backend Stores File**
â†’ Saves to `/data/uploads`
â†’ Registers file metadata in SQLite

3âƒ£ **Processing Request**
â†’ gRPC `process_query()`
â†’ Orchestrator forwards to appropriate MCP server (e.g., Whisper)

4âƒ£ **MCP Inference**
â†’ Runs local Whisper model
â†’ Returns JSON result

5âƒ£ **Response Back to UI**
â†’ gRPC response â†’ Tauri Rust â†’ React frontend

---

## 6. Folder Structure

```
project-root/
â”œâ”€â”€ src-tauri/
â”‚   â”œâ”€â”€ src/main.rs
â”‚   â”œâ”€â”€ proto/videoprocessor.proto
â”‚   â””â”€â”€ target/release/bundle/
â”‚
â”œâ”€â”€ python-backend/
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ grpc/
â”‚   â”‚   â”œâ”€â”€ video_processor_pb2.py
â”‚   â”‚   â””â”€â”€ video_processor_pb2_grpc.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ llama_agent.py
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â””â”€â”€ storage.py
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ whisper_tool.py
â”‚   â”‚   â”œâ”€â”€ detection_tool.py
â”‚   â”‚   â””â”€â”€ summarization_tool.py
â”‚   â”œâ”€â”€ data/uploads/
â”‚   â”œâ”€â”€ models/whisper/
â”‚   â”œâ”€â”€ models/yolo/
â”‚   â”œâ”€â”€ models/llama/
â”‚   â””â”€â”€ requirements.txt
```

---

## 7. Model Context Protocol (MCP)

* Each tool conforms to the MCP spec (`fastmcp` implementation).
* Orchestrator uses JSON-RPC 2.0 messaging for:

  * `initialize`
  * `tools/list`
  * `tools/call`
* The local Llama agent interprets tool schemas and calls the correct MCP server.

---

## 8. Local Testing

```bash
# Start Whisper MCP server
python python-backend/tools/whisper_tool.py

# Run Python orchestrator
python python-backend/server.py

# Run Tauri (Rust + React)
npm run tauri dev
```

You should see:

```
ðŸŽ· Whisper: Transcribing test.mp4 using base model...
```

---

## 9. Future Extensions

* âœ… Integrate YOLO MCP for object detection
* âœ… Add summarization MCP using local LLM
* âœ… Implement centralized ModelRegistry class
* á½¨0 Add health check / heartbeat system for MCP processes
* ðŸ§© Package everything into a single binary via PyInstaller + Tauri bundle
