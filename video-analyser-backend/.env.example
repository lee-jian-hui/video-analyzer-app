# ============================================
# Video Analyser Backend - Environment Config
# ============================================

# --------------------------------------------
# Model Backend Configuration (New System)
# --------------------------------------------
# Choose backend for each model type: "ollama", "local", "remote"

# Function calling model backend (for tool use)
FUNCTION_CALLING_BACKEND=ollama

# Chat model backend (for conversational responses)
CHAT_BACKEND=ollama

# --------------------------------------------
# Ollama Configuration
# --------------------------------------------
# Used when FUNCTION_CALLING_BACKEND=ollama or CHAT_BACKEND=ollama

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_FUNCTION_CALLING_MODEL=qwen3:0.6b
OLLAMA_CHAT_MODEL=qwen3:0.6b
OLLAMA_TEMPERATURE=0.1

# --------------------------------------------
# Local Transformers Configuration
# --------------------------------------------
# Used when FUNCTION_CALLING_BACKEND=local or CHAT_BACKEND=local
# Options: llama, codellama, qwen, qwen3, phi3

LOCAL_FUNCTION_CALLING_MODEL=qwen3
LOCAL_CHAT_MODEL=qwen3
LOCAL_TEMPERATURE=0.1

# Hardware settings for local models
DEVICE_MAP=auto
TORCH_DTYPE=auto
MAX_NEW_TOKENS=512

# --------------------------------------------
# Remote (Cloud API) Configuration
# --------------------------------------------
# Used when FUNCTION_CALLING_BACKEND=remote or CHAT_BACKEND=remote

REMOTE_PROVIDER=google_genai
REMOTE_MODEL_NAME=gemini-2.0-flash-lite
REMOTE_TEMPERATURE=0.0
GEMINI_API_KEY=your_gemini_api_key_here

# --------------------------------------------
# Video Processing Configuration
# --------------------------------------------

YOLO_MODEL_SIZE=yolov8n
OCR_LANGUAGE=eng
VIDEO_SAMPLE_INTERVAL=30

# --------------------------------------------
# Model Cache Directory
# --------------------------------------------

ML_MODEL_CACHE_DIR=./ml-models

# --------------------------------------------
# Logging Configuration
# --------------------------------------------

LOG_LEVEL=INFO
# LOG_FILE=app.log  # Uncomment to enable file logging
LOG_FORMAT=[%(asctime)s] %(name)s - %(levelname)s - %(message)s

# --------------------------------------------
# Agent Configuration
# --------------------------------------------

DEFAULT_EXECUTION_MODE=single
MAX_LLM_CALLS=10
ORCHESTRATOR_TIMEOUT=300
ENABLE_WORKFLOW_VISUALIZATION=true

# --------------------------------------------
# Advanced Settings
# --------------------------------------------

INFERENCE_TIMEOUT=720
HF_HUB_OFFLINE=false
TRANSFORMERS_OFFLINE=false

# --------------------------------------------
# Legacy Configuration (Deprecated)
# --------------------------------------------
# These still work but are deprecated. Use the new backend system above.

# USE_OLLAMA=true
# USE_LOCAL_FUNCTION_CALLING=true
# USE_LOCAL_CHAT=true
# USE_LOCAL_LLM=false
# FUNCTION_CALLING_MODEL_TYPE=qwen3
# CHAT_MODEL_TYPE=qwen3

# ============================================
# Quick Start Examples
# ============================================

# Example 1: All Ollama (Recommended)
# FUNCTION_CALLING_BACKEND=ollama
# CHAT_BACKEND=ollama

# Example 2: Hybrid (Ollama + Gemini)
# FUNCTION_CALLING_BACKEND=ollama
# CHAT_BACKEND=remote
# GEMINI_API_KEY=your_key

# Example 3: All Local (Desktop App)
# FUNCTION_CALLING_BACKEND=local
# CHAT_BACKEND=local
# LOCAL_FUNCTION_CALLING_MODEL=qwen3
# LOCAL_CHAT_MODEL=qwen3

# Example 4: All Remote (Cloud Only)
# FUNCTION_CALLING_BACKEND=remote
# CHAT_BACKEND=remote
# GEMINI_API_KEY=your_key
