from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
import json

# Define LLM
llm_cfg = {
    'model': 'qwen3:0.6b',  # Ollama model name format

    # Use the endpoint provided by Alibaba Model Studio:
    # 'model_type': 'qwen_dashscope',
    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),

    # Use a custom endpoint compatible with OpenAI API: (like ollama)
    'model_server': 'http://localhost:11434/v1',  # api_base (Ollama default port)
    'api_key': 'EMPTY',

    # Other parameters:
    # 'generate_cfg': {
    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;
    #         # Do not add: When the response has been separated by reasoning_content and content.
    #         'thought_in_content': True,
    #     },
}

# Define a simple adding tool for testing using qwen-agent's BaseTool
@register_tool('add_numbers')
class AddNumbers(BaseTool):
    description = 'Add two numbers together and return the sum.'
    parameters = [
        {
            'name': 'a',
            'type': 'number',
            'description': 'First number to add',
            'required': True
        },
        {
            'name': 'b',
            'type': 'number',
            'description': 'Second number to add',
            'required': True
        }
    ]

    def call(self, params: str, **kwargs) -> str:
        # Parse the parameters generated by the LLM
        params_dict = json.loads(params)
        a = params_dict['a']
        b = params_dict['b']
        result = a + b
        return json.dumps({'result': result}, ensure_ascii=False)

# Define video object detection tool
@register_tool('detect_objects_in_video')
class DetectObjectsInVideo(BaseTool):
    description = 'Detect objects in a video file using YOLO model. Returns detected objects with counts.'
    parameters = [
        {
            'name': 'confidence_threshold',
            'type': 'number',
            'description': 'Minimum confidence score for detections (0.0-1.0), default 0.5',
            'required': False
        }
    ]

    def call(self, params: str, **kwargs) -> str:
        # Parse parameters
        try:
            params_dict = json.loads(params) if params else {}
        except:
            params_dict = {}

        confidence_threshold = params_dict.get('confidence_threshold', 0.5)
        video_path = 'sample.mp4'  # Hardcoded video path

        try:
            from ultralytics import YOLO
            import cv2

            # Load YOLO model
            model = YOLO('./ml-models/yolo/yolov8n.pt')

            # Open video
            cap = cv2.VideoCapture(video_path)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)

            all_detections = []
            frame_num = 0

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                # Run inference on frame
                results = model(frame, conf=confidence_threshold, verbose=False)

                for r in results:
                    boxes = r.boxes
                    if boxes is not None:
                        for box in boxes:
                            class_id = int(box.cls[0])
                            class_name = model.names[class_id]
                            all_detections.append(class_name)

                frame_num += 1

            cap.release()

            # Count unique objects
            from collections import Counter
            object_counts = Counter(all_detections)

            result = {
                'video': video_path,
                'total_detections': len(all_detections),
                'frames_processed': frame_count,
                'unique_objects': dict(object_counts)
            }

            return json.dumps(result, ensure_ascii=False)

        except Exception as e:
            return json.dumps({'error': str(e)}, ensure_ascii=False)

# Define Tools
tools = [
    'add_numbers',  # Use the registered tool name
    'detect_objects_in_video',  # Video object detection tool
    # {'mcpServers': {  # You can specify the MCP configuration file
    #         'time': {
    #             'command': 'uvx',
    #             'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
    #         },
    #         "fetch": {
    #             "command": "uvx",
    #             "args": ["mcp-server-fetch"]
    #         }
    #     }
    # },
  # 'code_interpreter',  # Built-in tools
]

# Define Agent
bot = Assistant(llm=llm_cfg, function_list=tools)

# Streaming generation
messages = [{'role': 'user', 'content': 'I have a video file at sample.mp4. Identify which tool to use to analyze what objects are in this video.'}]
# messages = [{'role': 'user', 'content': 'What is 15 + 27? Use the add_numbers tool.'}]
# messages = [{'role': 'user', 'content': 'interpret thjis code : x = 2'}]
# messages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]
for responses in bot.run(messages=messages):
    pass
print(responses)
